{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the HDF5 Dataset\n",
    "\n",
    "* Objective is to transform from a **Hierarchical version** (folders and files) of a dataset to a more **compact** representation\n",
    "![HDF5 Compact Dataset](../../images/hdf5_dataset.png)\n",
    "\n",
    "* This involves putting all the images in a Zip file as numpy array along with their string and binary labels\n",
    "* We will also divide the dataset into 3 sections i.e. training (train), validation (dev) and testing (test)\n",
    "* We will also add a csv file containing all the information about the images i.e. image name, class, width, height and size to allow us to make statistics about the images\n",
    "\n",
    "##### Steps\n",
    "\n",
    "1. Compress all images into one HDF5 dataset file\n",
    "2. Split the dataset into train/dev/test datasets\n",
    "\n",
    "#### Why HDF5 representation?\n",
    "\n",
    "* More compact representation of the dataset - only one file containing all images in a numpy array format\n",
    "* Much faster to load (in Google Colab)\n",
    "* Fixed partition into train, dev/test datasets \n",
    "* Make possible to compare between models - since we are using the same training, testing and validation data. This will allow for a fair comparison of the performance of the different deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

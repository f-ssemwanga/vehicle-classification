{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the HDF5 Dataset\n",
    "\n",
    "* Objective is to transform from a **Hierarchical version** (folders and files) of a dataset to a more **compact** representation\n",
    "![HDF5 Compact Dataset](../../images/hdf5_dataset.png)\n",
    "\n",
    "* This involves putting all the images in a Zip file as numpy array along with their string and binary labels\n",
    "* We will also divide the dataset into 3 sections i.e. training (train), validation (dev) and testing (test)\n",
    "* We will also add a csv file containing all the information about the images i.e. image name, class, width, height and size to allow us to make statistics about the images\n",
    "\n",
    "##### Steps\n",
    "\n",
    "1. Compress all images into one HDF5 dataset file\n",
    "2. Split the dataset into train/dev/test datasets\n",
    "\n",
    "#### Why HDF5 representation?\n",
    "\n",
    "* More compact representation of the dataset - only one file containing all images in a numpy array format\n",
    "* Much faster to load (in Google Colab)\n",
    "* Fixed partition into train, dev/test datasets \n",
    "* Make possible to compare between models - since we are using the same training, testing and validation data. This will allow for a fair comparison of the performance of the different deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting dataset into HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required modules\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decide the image size\n",
    "\n",
    "* 224 is used in common deep neural network models, but we can choose any other size.\n",
    "* Large image sizes would require more memory for the deep learning model and it will take longer to execute\n",
    "* Smaller image sizes will lead to faster execution however we would loose a lot of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the path for the location of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../dataset\n",
      "../../hDF5DatasetVehicle-type-dataset-SIZE224.hdf5\n"
     ]
    }
   ],
   "source": [
    "#Change the paths below according to the system on which the code is being run\n",
    "# good to have the size in the name in case you create datasets with different image sizes.\n",
    "DATASET_PATH = '../../dataset'\n",
    "H5DATASET_FOLDER = '../../hDF5Dataset' #created at run time\n",
    "TARGET_HDF5_DATASET_PATH =f'{H5DATASET_FOLDER}Vehicle-type-dataset-SIZE{IMAGE_SIZE}.hdf5' #created when the code is run\n",
    "OUTPUT_STATS_FILE = f'{H5DATASET_FOLDER} vehicle-type-dataset-SIZE{IMAGE_SIZE}.hdf5.csv'\n",
    "\n",
    "print(DATASET_PATH)\n",
    "print(TARGET_HDF5_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the max label length to have the longest in the current dataset\n",
    "#Value must be equal or greater to the length of the label with the largest string length\n",
    "class_label_string_length = \"S30\" # string with 30 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Utilise the utilities lib \n",
    "* ![Understanding the load_rgb_data() method](../../images/Loading_Data_Method.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'motorcycle']\n"
     ]
    }
   ],
   "source": [
    "classes  = os.listdir(DATASET_PATH)\n",
    "classes.sort()\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../dataset\n",
      "Loading images...\n",
      "Loaded 50 images so far ...\n",
      "Loaded 100 images so far ...\n",
      "Loaded 150 images so far ...\n",
      "Loaded 200 images so far ...\n",
      "Loaded 250 images so far ...\n",
      "Loaded 300 images so far ...\n",
      "Loaded 350 images so far ...\n",
      "Loaded 400 images so far ...\n",
      "Loaded 450 images so far ...\n",
      "Loaded 500 images so far ...\n",
      "Loaded 550 images so far ...\n",
      "Loaded 600 images so far ...\n",
      "Loaded 650 images so far ...\n",
      "Loaded 700 images so far ...\n",
      "Loaded 750 images so far ...\n",
      "Loaded 800 images so far ...\n",
      "Loaded 850 images so far ...\n",
      "Loaded 900 images so far ...\n",
      "Loaded 950 images so far ...\n",
      "Loaded 1000 images so far ...\n",
      "Loaded 1050 images so far ...\n",
      "Loaded 1100 images so far ...\n",
      "Loaded 1150 images so far ...\n",
      "Loaded 1200 images so far ...\n",
      "Loaded 1250 images so far ...\n",
      "Loaded 1300 images so far ...\n",
      "Loaded 1350 images so far ...\n",
      "Number of images loaded: 1362\n",
      "Dataset shuffled.\n"
     ]
    }
   ],
   "source": [
    "# use methods from the utils_lib\n",
    "# set the path for the library\n",
    "\n",
    "import utils_lib\n",
    "from utils_lib import *\n",
    "sys.path.append(\"./\")\n",
    "print(DATASET_PATH)\n",
    "\n",
    "\n",
    "data, labels =load_rgb_data(DATASET_PATH,IMAGE_SIZE,directory_depth=0, max_number_of_images=None,shuffle_data=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
